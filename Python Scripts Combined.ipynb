{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9faa6e72",
   "metadata": {},
   "source": [
    "## AIR Rerun EP (working).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8947301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# AIR Rerun EP\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyodbc\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "#create EP table\n",
    "cols = ['ID','ANLSID','NAME','EPTYPE','PERSPCODE','RP','PERSPVALUE']\n",
    "ep_table = pd.DataFrame(columns=cols).set_index('ID')\n",
    "ep_table.head()\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "server = 'DFWCAT-TS3SQL1'\n",
    "db = 'RV_CM_locAAL_TSR'\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server}; SERVER='+server+';DATABASE='+db+';')\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "#Determine Loss to use\n",
    "AnalysisSID_query = \"select * from tAnalysisResult\"\n",
    "AnalysisSID = pd.read_sql(AnalysisSID_query,cnxn)\n",
    "AnalysisSID.head()\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "AnalysisSID = 6\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "ResultSplitLevel = 'State'\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "#Determine loss perspectives saved\n",
    "perspectives_query = \"SELECT distinct FinancialPerspectiveCode from t6_LOSS_AnnualEP\" #update loss table number\n",
    "perspectives = pd.read_sql(perspectives_query,cnxn)\n",
    "\n",
    "perspectives = pd.DataFrame(perspectives)\n",
    "perspectives.head()\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "#Get details of the analysis\n",
    "AnalysisName_query = \"SELECT AnalysisName FROM tAnalysisResult WHERE ResultSID=6\" #update loss table number\n",
    "AnalysisName = pd.read_sql(AnalysisName_query,cnxn).iloc[0,0]\n",
    "OuputTypeCode_query = 'SELECT OutputTypeCode FROM tLossAnalysisOption WHERE ResultSID=6' #update loss table number\n",
    "OutputTypeCode = pd.read_sql(OuputTypeCode_query,cnxn).iloc[0,0]\n",
    "GeoLevelCode_query = 'SELECT GeoLevelCode FROM tLossAnalysisOption WHERE ResultSID=6'\n",
    "GeoLevelCode = pd.read_sql(GeoLevelCode_query,cnxn).iloc[0,0]\n",
    "print(AnalysisName)\n",
    "print(OutputTypeCode)\n",
    "print(GeoLevelCode)\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "#Set AnalysisLevel\n",
    "if OutputTypeCode == 'LOC':\n",
    "    AnalysisLevel = 1\n",
    "elif OutputTypeCode == 'GEOL' and GeoLevelCode == 'SUBA':\n",
    "    AnalysisLevel = 2\n",
    "elif OutputTypeCode == 'GEOL' and GeoLevelCode == 'AREA':\n",
    "    AnalysisLevel = 3\n",
    "elif OuputTypeCode == 'CONGEAL' and GeoLevelCode == 'SUBA':\n",
    "    AnalysisLevel = 4\n",
    "elif OutputTypeCode == 'EAGEOL' and GeoLevelCode == 'SUBA':\n",
    "    AnalysisLevel = 5\n",
    "elif OuputTypeCode == 'CONGEAL' and GeoLevelCode == 'AREA': \n",
    "    AnalysisLevel = 6\n",
    "elif OutputTypeCode == 'EAGEOL' and GeoLevelCode == 'AREA':\n",
    "    AnalysisLevel = 7\n",
    "elif OutputTypeCode == 'EA':\n",
    "    AnalysisLevel = 8\n",
    "elif OutputTypeCode == 'CON':\n",
    "    AnalysisLevel = 9\n",
    "else:\n",
    "    AnalysisLevel = None\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "#Set ResultTable\n",
    "\n",
    "if AnalysisLevel == 1:\n",
    "    ResultTable = 't'+str(AnalysisSID)+'_LOSS_ByLocation'\n",
    "elif AnalysisLevel == 2:\n",
    "    ResultTable = 't'+str(AnalysisSID)+'_LOSS_ByGeo'\n",
    "elif AnalysisLevel == 4:\n",
    "    ResultTable = 't'+str(AnalysisSID)+'_LOSS_ByContractGeo'\n",
    "elif AnalysisLevel == 5:\n",
    "    ResultTable = 't'+str(AnalysisSID)+'_LOSS_ByExposureAttributeGeo'\n",
    "elif AnalysisLevel == 8:\n",
    "    ResultTable = 't'+str(AnalysisSID)+'_LOSS_ByByExposureAttribute'\n",
    "elif AnalysisLevel == 9:\n",
    "    ResultTable = 't'+str(AnalysisSID)+'_LOSS_ByContract'\n",
    "else:\n",
    "    ResultTable = None\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "#Set current max SID\n",
    "CurrentMaxSID_query = \"SELECT LastSID FROM tSIDControl WHERE TableName='tAnalysisResult'\"\n",
    "CurrentMaxSID = pd.read_sql(CurrentMaxSID_query,cnxn).iloc[0,0]\n",
    "print(CurrentMaxSID)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "ResultTable\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "##Now we do the by State split\n",
    "#Drop temp_geo table if it exists\n",
    "# drop_temp_geo_query = 'drop table temp_geo'\n",
    "# drop_temp_geo = pd.read_sql(drop_temp_geo_query,cnxn)\n",
    "\n",
    "#Insert all states into temp_geo\n",
    "if AnalysisLevel == 1:\n",
    "    ResultTable = 't6_LOSS_ByLocation'+' z INNER JOIN t'+str(AnalysisSID)+'_LOSS_DimLocation a ON z.LocationSID=a.LocationSID'\n",
    "else:\n",
    "    ResultTable = 't6_LOSS_ByLocation' + ' a'\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#couldn't get this to work\n",
    "# temp_geo_query = '''SELECT AreaCode State,AreaCode Geo into temp_geo FROM t6_LOSS_ByLocation z INNER JOIN t6_LOSS_DimLocation a ON z.LocationSID=a.LocationSID INNER JOIN AIRGeography.dbo.tGeography g ON a.GeographySID=g.GeographySID GROUP BY AreaCode'''\n",
    "# temp_geo = pd.read_sql(temp_geo_query,cnxn)\n",
    "# temp_geo = pd.DataFrame(temp_geo)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#temp_geo.to_sql(temp_geo, con=cnxn)\n",
    "#this killed kernal so created table in SSMS\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "temp_geo_query = 'select * from temp_geo'\n",
    "temp_geo = pd.read_sql(temp_geo_query,cnxn)\n",
    "print(temp_geo)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#insert all counties into temp_geo\n",
    "# temp_geo_query = '''SELECT AreaCode State,SubAreaName Cpunty, AreaCode+\"_\"+SubAreaName Geo into temp_geo FROM t6_LOSS_ByLocation z INNER JOIN t6_LOSS_DimLocation a ON z.LocationSID=a.LocationSID INNER JOIN AIRGeography.dbo.tGeography g ON a.GeographySID=g.GeographySID GROUP BY AreaCode'''\n",
    "# temp_geo = pd.read_sql(temp_geo_query,cnxn)\n",
    "# temp_geo = pd.DataFrame(temp_geo)\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "##Create Master Loss Table\n",
    "#Loss By Event\n",
    "\n",
    "#Couldn't get this to work either\n",
    "# temp_eventloss_query = '''SELECT CatalogTypeCode,EventID,ModelCode,YearID,PerilSetCode,Geo, \n",
    "# sum(GroundUpLoss) GroundUpLoss,sqrt(sum(GroundUpSD*GroundUpSD)) GroundUpSD,sum(GroundUpMaxLoss) GroundUpMaxLoss,\n",
    "# sum(GrossLoss) GrossLoss,sqrt(sum(GrossSD*GrossSD)) GrossSD,sum(GrossMaxLoss) GrossMaxLoss,\n",
    "# sum(NetOfPreCatLoss) NetOfPreCATLoss\n",
    "# INTO temp_eventloss\n",
    "# FROM t6_LOSS_ByLocation z INNER JOIN t6_LOSS_DimLocation a ON z.LocationSID=a.LocationSID INNER JOIN AirGeography.dbo.tGeography g ON a.GeographySID=g.GeographySID\n",
    "# INNER JOIN temp_geo tg ON g.AreaCode=tg.State\n",
    "# GROUP BY CatalogTypeCode,EventID,ModelCode,YearID,PerilSetCode,Geo'''\n",
    "# temp_eventloss = pd.read_sql(temp_eventloss_query,cnxn)\n",
    "# print(temp_eventloss)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print(temp_eventloss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f98202b",
   "metadata": {},
   "source": [
    "## Bulk_Insert_SQL.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "server = 'GCVPW-RDB-HAPRZ' #edit server name\n",
    "db = 'AccumTesting_v21_EDM' #edit db name\n",
    "\n",
    "params = urllib.parse.quote_plus('DRIVER={SQL SERVER};SERVER= '+server+';DATABASE='+db+';')\n",
    "\n",
    "df = pd.read_csv('C:\\Python\\Colonnade_Galahad_Exposure.txt', sep = '\\t')\n",
    "\n",
    "engine = create_engine(\"mssql+pyodbc:///?odbc_connect={}\".format(params), fast_executemany=True)\n",
    "\n",
    "df.to_sql('Colonnade_Galahad_Exposure', con = engine, index=False, if_exists='append',schema='dbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd70601",
   "metadata": {},
   "source": [
    "## Convert_CSV_to_TXT.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ff1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "directory = r'C:\\Users\\u1207199\\Downloads\\Arium\\Everest\\ELDs'\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "for in_path in Path(directory).glob('*.csv'):\n",
    "    out_path = in_path.with_suffix('.txt')\n",
    "    with in_path.open('r') as fin, out_path.open('w') as fout:\n",
    "        reader = csv.DictReader(fin)\n",
    "        writer = csv.DictWriter(fout, reader.fieldnames, delimiter='\\t')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(reader)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce8c0ee",
   "metadata": {},
   "source": [
    "## Export_SQL_Table.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be5624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "server = 'GCVPW-TDB-DZ5GK' #add server name here\n",
    "DB= 'AIRResult_Secura_06302023_TSv10' #add database name here\n",
    "\n",
    "engine = create_engine(f'mssql+pyodbc://{server}/{DB}?driver=SQL+Server')\n",
    "\n",
    "sql = '''select * from \n",
    "\t\t\t\tSECURA_SCS_YLT\t--add table name here\n",
    "\t\t\t'''\n",
    "\n",
    "df = pd.read_sql_query(sql, engine)\n",
    "\n",
    "df.to_csv(r'C:\\Python\\SECURA_SCS_YLT.csv', index=False)\t#add file name here\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import pyodbc\n",
    "\n",
    "# server = 'GCVPW-TDB-P5BQQ' #add server name here\n",
    "# DB= 'Hagerty_20221231_v10_TSE' #add database name here\n",
    "\n",
    "\n",
    "# conn = pyodbc.connect('DRIVER={SQL Server}; SERVER='+server+';DATABASE='+DB+';')\n",
    "\n",
    "# sql = pd.read_sql_query('''select * from \n",
    "# \t\t\t\tz_GCAP_Upload\t--add table name here\n",
    "# \t\t\t''', conn)\n",
    "\n",
    "# df = pd.DataFrame(sql)\n",
    "# df.to_csv(r'C:\\Python\\Hagerty_20221231_GCAP_ExposureOnly.csv', index=False)\t#add file name here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a584a",
   "metadata": {},
   "source": [
    "## Import Data to SQL Database.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad756a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Excel or CSV file to SQL Server database\n",
    "\n",
    "import pandas as pd\n",
    "import pypyodbc as odbc\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import url\n",
    "\n",
    "server = '' #add server name here\n",
    "database = '' #add database here\n",
    "driver = '{SQL Server}'\n",
    "\n",
    "connection_url = URL.create('mssql+pyodbc', query={'odbc_connect': f'DRIVER{driver};SERVER={server};DATABASE={database}'})\n",
    "engine = create_engine(connection_url, module=odbc)\n",
    "\n",
    "file = '' #add excel or csv file here\n",
    "df = pd.read_excel(file, sheet_name=None)\n",
    "for sheet_name, table in df.items():\n",
    "    if table.empty:\n",
    "        continue\n",
    "    try:\n",
    "        df.to_sql(sheet_name, con=engine, if_exists='replace', index=False, dtype={'user_id': sqlalchemy.types.VARCHAR(length=255)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb632b0",
   "metadata": {},
   "source": [
    "## Import Data to SQL.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9348875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "server_name = 'DFWCAT-TS5SQL2'\n",
    "database_name = 'UticaNational_20220930_v9_TSE_AIRExp'\n",
    "\n",
    "# set up the connection to the SQL Server database\n",
    "engine = create_engine(f'mssql+pyodbc://{server_name}/{database_name}?driver=ODBC+Driver+17+for+SQL+Server')\n",
    "\n",
    "# read the .csv file into a pandas DataFrame\n",
    "df = pd.read_csv('AIR_ConstScheme_ForSQL.csv')\n",
    "\n",
    "# write the data from the DataFrame to the SQL Server table\n",
    "df.to_sql('table_name', engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f6b4b6",
   "metadata": {},
   "source": [
    "## pandas - applying bands.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d59de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yr_blt(data):\n",
    "    if data.Year_Built <= 1975:\n",
    "        return 'Pre-1975'\n",
    "    elif data.Year_Built >1975 and data.Year_Built <= 1994:\n",
    "        return '1976-1994'\n",
    "    elif data.Year_Built > 1994 and data.Year_Built <= 1997:\n",
    "        return '1995-1997'\n",
    "    elif data.Year_Built > 1997 and data.Year_Built <= 2001:\n",
    "        return '1998-2001'\n",
    "    elif data.Year_Built > 2001 and data.Year_Built <=2008:\n",
    "        return '2002-2008'\n",
    "    elif data.Year_Built > 2008:\n",
    "        return 'Post-2009'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "df['Year_Band'] = df.apply(yr_blt, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636dc93",
   "metadata": {},
   "source": [
    "## Print Full Pandas DF.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa98de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "\n",
    "#OR\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe494ee",
   "metadata": {},
   "source": [
    "RMS Layer Stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import betainc\n",
    "from math import sqrt\n",
    "\n",
    "def RMSLayerStdev(AttPt, limit, mean, stddev, Exposure):\n",
    "    if Exposure == 0 or stddev == 0:\n",
    "        return 0\n",
    "\n",
    "    mu = mean / Exposure\n",
    "    sigma = stddev / Exposure\n",
    "\n",
    "    Alpha = (mu ** 2 * (1 - mu) / sigma ** 2) - mu\n",
    "    Beta = Alpha * (1 - mu) / mu\n",
    "\n",
    "    L2 = min(1, AttPt / Exposure)\n",
    "    L3 = min(1, (AttPt + limit) / Exposure)\n",
    "\n",
    "    r2 = mu * (1 - betainc(Alpha + 1, Beta, L2)) - L2 * (1 - betainc(Alpha, Beta, L2))\n",
    "    r3 = mu * (1 - betainc(Alpha + 1, Beta, L3)) - L3 * (1 - betainc(Alpha, Beta, L3))\n",
    "\n",
    "    xsq2 = (Alpha + 1) * mu * (1 - betainc(Alpha + 2, Beta, L2)) / (Alpha + Beta + 1) - 2 * L2 * mu * (1 - betainc(Alpha + 1, Beta, L2)) + L2 ** 2 * (1 - betainc(Alpha, Beta, L2))\n",
    "    xsq3 = (Alpha + 1) * mu * (1 - betainc(Alpha + 2, Beta, L3)) / (Alpha + Beta + 1) - 2 * L3 * mu * (1 - betainc(Alpha + 1, Beta, L3)) + L3 ** 2 * (1 - betainc(Alpha, Beta, L3))\n",
    "\n",
    "    lxsq = xsq2 - xsq3 - 2 * (L3 - L2) * r3\n",
    "\n",
    "    return sqrt(max(0, (lxsq - (r2 - r3) ** 2))) * Exposure\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
